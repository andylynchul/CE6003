{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CE6003_20208189_ Etivity_assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andylynchul/CE6003/blob/main/Week%202/CE6003_20208189__Etivity_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMd1Rc3eDwBP"
      },
      "source": [
        "# Etivity 2: Deep Convolutional Neural Network on CIFAR-10 Dataset\n",
        "\n",
        "In this assignment, we will gain some practical experience of coding a deep convolutional neural network in Tensorflow. The simplest way to code a network is to use the High level Keras API within Tensorflow 2.5.  \n",
        "\n",
        "## (a) Introduction\n",
        "\n",
        " In this assignment to reduce training time and computation, we will train our network using the simple [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n",
        "![link text](https://paperswithcode.com/media/datasets/CIFAR-10-0000000431-b71f61c0_U5n3Glr.jpg)\n",
        "\n",
        "The goal of this Etivity is to build and train your own Deep Convolutional Neural Network. This notebook contains standard keras/Tensorflow code to perform data processing, training set up and testing metrics, as outlined in the steps below. This will allow you to focus on developing the architecture & code for your network. \n",
        "\n",
        "1. Load Dataset - We will import the CIFAR-10 Dataset using the Tensorflow Data Set API.\n",
        "2. Prepare Data - We will slightly modify the dataset before it is sent to the model for training.\n",
        "3. Model Coding - We will write code to build the model using the Tensorflow keras API.\n",
        "  * **You will code your neural network model in this section**\n",
        "4. Compile Model - We will complile the model and verify that it has been correctly constructed.\n",
        "5. Train Model - We will train the model using the imported dataset.\n",
        "6. Test Model - We test the model on the training data and obtain a classification report and confusion matrix.\n",
        "\n",
        "The principal resource for understanding the basics of how to code & train Deep Convolutional Neural Network architectures in Tensorflow/keras are the Tensorflow tutorials. Tutorials on [image classification](https://www.tensorflow.org/tutorials/images/classification) and [data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) are useful for this Etivity.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKjhi09Kfwem"
      },
      "source": [
        "\n",
        "## (b) Notes on Architecture selection & development\n",
        "\n",
        "A key task in this Etivity is selecting an appropriate architecture. You can base your archicture on any of the types seen in the lecture notes or published literature. It is however recommended to ensure you can get a standard CNN type architecture to successfully train before attempting a more complex architecture or adding further enhancements to your network to improve performance.\n",
        "   Many published networks are designed to work with larger sized input images (e.g. 224 x 224). Often large (e.g. 7 x 7) convolution kernels and pooling layers are first applied to the input image, which agressively reduce the spatial dimensions of the network. As the CIFAR-10 dataset only has 32 x 32 images, it is recommended that just an intial 3 x 3 convolution is applied to the input image (as shown in the figure below), which will preserve the spatial dimensions of the input, before any other layers are added. It is expected that as you add more layers to the network the spatial dimensions will reduce and there will also be a corresponding increase the number of channels of the output feature map. The architecture will end with a [global average pooling layer](https://arxiv.org/pdf/1312.4400.pdf) and a fully-connected layer (called a *dense* layer in Keras/Tensorflow) with softmax activation which gives us our 10 class predictions. A skeleton code has been provided for any general architure using this structure in section 3.\n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/Architecture_outline.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1AqzPcvPIMZ"
      },
      "source": [
        "# Information on Load Dataset\n",
        "\n",
        "In this assignment, we will be using the [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.htmlhttps://). CIFAR-10 is often used as a \"Hello World\" dataset that is often used to ensure a network architecture is working before moving on to training with more complex datasets.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (airplane, automobile, **bird**, cat, deer, dog, frog, horse, ship, truck), with 6,000 images per class. There are 50,000 training images and 10,000 test images. \n",
        "\n",
        "We will use the [Tensorflow dataset](https://www.tensorflow.org/datasets/catalog/cifar10) API to download the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usCVtcvZ1jF"
      },
      "source": [
        "# Library Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCZcWwiaDOGa"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D, Input, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Added by me\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.datasets import cifar10 \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import load_model\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from keras.layers import Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhX8sJMkXg6B"
      },
      "source": [
        "# Global Variable Store"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut3hQDplvzFK"
      },
      "source": [
        "# Globals\n",
        "batch_size = 32 # Default Batch size (can be adjusted)\n",
        "H_trg =32       # Image Height (fixed)\n",
        "W_trg =32       # image Width (fixed)\n",
        "\n",
        "# Labels corresponding to categories\n",
        "label_str = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_AX__0cXjxm"
      },
      "source": [
        "# Global Function Store"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATKgW5H9JtZ_"
      },
      "source": [
        "# Functions\n",
        "def load_data():\n",
        "    # Function to laod Data set\n",
        "    ds,info= tfds.load(\"cifar10\",with_info=True)\n",
        "\n",
        "    raw_train = tfds.load('cifar10', split='train[0%:90%]')\n",
        "    raw_val = tfds.load('cifar10', split='train[91%:100%]')\n",
        "    raw_test = tfds.load('cifar10', split='test')\n",
        "\n",
        "    return raw_train, raw_val, raw_test\n",
        "\n",
        "# alternative method to load the data.\n",
        "def load_data_alt():\n",
        "    # load dataset\n",
        "    (train_X, train_Y), (testX, testY) = cifar10.load_data()\n",
        "    \n",
        "    # generate Training & Validation set\n",
        "    trainX, validateX, trainY, validateY = train_test_split(train_X, train_Y, train_size=0.8)\n",
        "    \n",
        "    # one hot encode target values\n",
        "    trainY = to_categorical(trainY)\n",
        "    validateY = to_categorical(validateY)\n",
        "    testY = to_categorical(testY)\n",
        "    \n",
        "    return trainX, trainY, validateX, validateY, testX, testY\n",
        "\n",
        "# original data_pipe to convert pixels to floats to allow normalisation\n",
        "def data_pipe(image,label):\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
        "    image = image-0.5\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return image,label\n",
        "\n",
        "# update data_pipe to convert pixels to floats to allow normalisation\n",
        "def images_prep(train,val,test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    val_norm = val.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    val_norm = val_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, val_norm, test_norm\n",
        "\n",
        "# prepa single image for classification\n",
        "def image_prep(filename):\n",
        "    # load the image (using lib defined above)\n",
        "    img = load_img(filename, target_size=(H_trg, W_trg))\n",
        "    # convert to array (using lib defined above)\n",
        "    img = img_to_array(img)\n",
        "    # reshape into a single sample with 3 channels\n",
        "    img = img.reshape(1, 32, 32, 3)\n",
        "    # prepare pixel data\n",
        "    img = img.astype('float32')\n",
        "    img = img / 255.\n",
        "\n",
        "    return img\n",
        "\n",
        "# visulaise the confusion matrix for testing routine\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(label_str))\n",
        "    plt.xticks(tick_marks, label_str, rotation=45)\n",
        "    plt.yticks(tick_marks, label_str)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    print(history.history.keys())\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,2.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6P8JJq3XqBc"
      },
      "source": [
        "# Model Definition & Compilation\n",
        "\n",
        "Now that we have defined our functions to create the model, we'll instantiate the model and compile it.  Note that the compiling step in Keras, also configures the model for training. We define  loss function, the optimizer and metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMtWn8FE3Emz"
      },
      "source": [
        "# Model Definition\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    #data augmentaton - this embeds pre-processing in the model. better to apply to the data\n",
        "    #model.add(layers.RandomFlip())\n",
        "    #model.add(layers.RandomRotation(0.2))\n",
        "\n",
        "    #layer 1\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    #layer 2\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    #layer 2\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    #layer 3\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    # flatten\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Activation(activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # compile model\n",
        "    # opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    opt = Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzfvo2fb5qzj"
      },
      "source": [
        "# Model Definition\n",
        "def define_model_new():\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Conv => MaxPool\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # Conv => MaxPool\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # Conv => MaxPool\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # FC => SoftMax\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # set callbacks\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "    mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    # compile model\n",
        "    # opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    opt = Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], callback=[es,mc])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDtlSM7ZXuYZ"
      },
      "source": [
        "# Model Training Regime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzfMkJAUgT5l"
      },
      "source": [
        "We will now train the complied model on the cifar10 dataset using the tensorflow keras `model.fit` method. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1QmKxSXvYr8"
      },
      "source": [
        "# Model Training Regime\n",
        "def train_model(epochs_n=10, batch_size_n=64, save_model=False, aug=False):\n",
        "\t\t# load dataset\n",
        "\t\ttrainX, trainY, valX, valY, testX, testY = load_data_alt()\n",
        "\t\t# prepare pixel data\n",
        "\t\ttrainX, valX, testX = images_prep(trainX, valX, testX)\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\n",
        "\t\tif aug:\n",
        "\t\t\t# create data generator\n",
        "\t\t\t# datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\t \t\tdatagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.2, shear_range=0.2, zoom_range=0.3, horizontal_flip=True)\n",
        "\t\t\t# prepare iterator\n",
        "\t\t\tit_train = datagen.flow(trainX, trainY, batch_size=batch_size_n)\n",
        "\t\t\t# fit model\n",
        "\t\t\tsteps = int(trainX.shape[0] / batch_size_n)\n",
        "\t\t\t\n",
        "\t\t\thistory = model.fit(it_train, steps_per_epoch=steps, epochs=epochs_n, validation_data=(valX, valY), verbose=1)\n",
        "\t\telse:\n",
        "\t\t\t# fit model\n",
        "\t\t\thistory = model.fit(trainX, trainY, epochs=epochs_n, batch_size=batch_size_n, validation_data=(valX, valY), verbose=1)\n",
        "\t\t\n",
        "\t\t# evaluate model\n",
        "\t\t_, acc = model.evaluate(testX, testY, verbose=2)\n",
        "\t\t# save model if final\n",
        "\t\tif save_model:\n",
        "\t\t\tmodel.save('final_model.h5')\n",
        "\t\t#print accuracy\n",
        "\t\tprint('> %.3f' % (acc * 100.0))\n",
        "\t\t# learning curves\n",
        "\t\tsummarize_diagnostics(history)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5k8FaI6XzkY"
      },
      "source": [
        "# Model Fitness Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e16_Awsoeg8"
      },
      "source": [
        "We will iterate through the test data and analyse the results using tools from sklearn. We create a classification report, a confusion matrix and also plot a few examples from the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bqW2MDKJIu2"
      },
      "source": [
        "# Test Model\n",
        "def test_model(model, confusion=True, summary=True, plots=True):\n",
        "    n_test = 100 # number of batches to use\n",
        "    store_predictions = []\n",
        "    store_labels = []\n",
        "    for image_batch,label_batch in test_dataset.take(n_test):\n",
        "      predictions = model.predict_on_batch(image_batch) \n",
        "      predictions = tf.math.argmax(predictions,axis=1)\n",
        "      store_predictions.append(predictions)\n",
        "      store_labels.append(label_batch)\n",
        "    y_pred = np.squeeze(np.reshape(store_predictions,(1,n_test*batch_size)))\n",
        "    y_true = np.squeeze(np.reshape(store_labels,(1,n_test*batch_size)))\n",
        "\n",
        "    if confusion:\n",
        "      # Generate Confusion Matrix\n",
        "      cm = confusion_matrix(y_true, y_pred)\n",
        "      # Visualise Confusion Matrix\n",
        "      print(plot_confusion_matrix(cm))\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "    if summary:\n",
        "      # Generate Classification Report\n",
        "      print(classification_report(y_true, y_pred, target_names=label_str) + \"\\n\\n\")\n",
        "\n",
        "    if plots:\n",
        "      #Retrieve a batch of images from the test set\n",
        "      image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
        "      predictions = model.predict_on_batch(image_batch)\n",
        "      #print(np.shape(predictions))\n",
        "      # Apply a sigmoid since our model returns logits\n",
        "      predictions = tf.math.argmax(predictions,axis=1)\n",
        "      #print(np.shape(predictions))\n",
        "      #print('Predictions:\\n', predictions.numpy())\n",
        "\n",
        "      i=0\n",
        "      n_plots = 12 # number of plots\n",
        "      f, axarr = plt.subplots(1,n_plots,figsize=(20,10))\n",
        "\n",
        "      for image in image_batch[0:n_plots,:,:,:]:  # Only take a single example\n",
        "        axarr[i].imshow(image[:,:,:]+0.5)\n",
        "        axarr[i].axis('off')\n",
        "        \n",
        "        color = ('black' if predictions[i] == int(label_batch[i]) else 'red') \n",
        "        axarr[i].set_title(label_str[int(predictions[i])],fontsize='small', color=color)\n",
        "        i = i+1\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFBL_LsX7M0"
      },
      "source": [
        "# Live Image Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt0JdGO0Nwhn"
      },
      "source": [
        "# Test image on Final Model\n",
        "def classify_image(model):\n",
        "    # allow user to uplaod images\n",
        "    uploaded = files.upload()\n",
        "    #test_img_prepped = image_prep(test_img)\n",
        "    for fn in uploaded.keys():\n",
        "      test_img = image_prep(fn)\n",
        "      # predict the class\n",
        "      y_pred = model.predict(test_img)\n",
        "      y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "      show_img = Image.open(BytesIO(uploaded[fn]))\n",
        "\n",
        "      plt.imshow(show_img)\n",
        "      plt.show() \n",
        "      \n",
        "      print(\"We think this is of class : \",label_str[np.argmax(y_pred[0])])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7WSWpvXNQH"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ysWXeOk94tAi",
        "outputId": "b165252c-2373-4b86-e891-efdc08ed4c68"
      },
      "source": [
        "train_model(epochs_n=500, batch_size_n=64, save_model=True, aug=True)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_144 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_105 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_142 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_145 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_106 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_143 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_146 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_107 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_144 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_147 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_108 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_145 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_148 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_109 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_146 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_149 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_110 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_147 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_150 (Conv2D)          (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_111 (Activation)  (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_148 (Bat (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_151 (Conv2D)          (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_112 (Activation)  (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_149 (Bat (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_75 (MaxPooling (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_113 (Activation)  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_150 (Bat (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,442,090\n",
            "Trainable params: 1,439,658\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "625/625 [==============================] - 19s 29ms/step - loss: 1.8085 - accuracy: 0.3614 - val_loss: 1.8799 - val_accuracy: 0.3892\n",
            "Epoch 2/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 1.2628 - accuracy: 0.5487 - val_loss: 1.8531 - val_accuracy: 0.4679\n",
            "Epoch 3/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 1.0477 - accuracy: 0.6336 - val_loss: 1.6091 - val_accuracy: 0.4916\n",
            "Epoch 4/500\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.9191 - accuracy: 0.6823 - val_loss: 1.3561 - val_accuracy: 0.5855\n",
            "Epoch 5/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.8411 - accuracy: 0.7135 - val_loss: 1.0189 - val_accuracy: 0.6736\n",
            "Epoch 6/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.7882 - accuracy: 0.7329 - val_loss: 0.8166 - val_accuracy: 0.7254\n",
            "Epoch 7/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.7403 - accuracy: 0.7503 - val_loss: 0.8004 - val_accuracy: 0.7355\n",
            "Epoch 8/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.7073 - accuracy: 0.7634 - val_loss: 0.8305 - val_accuracy: 0.7328\n",
            "Epoch 9/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.6749 - accuracy: 0.7750 - val_loss: 0.9181 - val_accuracy: 0.7116\n",
            "Epoch 10/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.6471 - accuracy: 0.7846 - val_loss: 0.8809 - val_accuracy: 0.7274\n",
            "Epoch 11/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.6247 - accuracy: 0.7915 - val_loss: 0.6538 - val_accuracy: 0.7851\n",
            "Epoch 12/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.6042 - accuracy: 0.7991 - val_loss: 0.6521 - val_accuracy: 0.7869\n",
            "Epoch 13/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.5826 - accuracy: 0.8062 - val_loss: 0.7347 - val_accuracy: 0.7642\n",
            "Epoch 14/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.5599 - accuracy: 0.8135 - val_loss: 0.5708 - val_accuracy: 0.8148\n",
            "Epoch 15/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.5535 - accuracy: 0.8171 - val_loss: 0.6270 - val_accuracy: 0.7938\n",
            "Epoch 16/500\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.5332 - accuracy: 0.8236 - val_loss: 0.5216 - val_accuracy: 0.8255\n",
            "Epoch 17/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.5194 - accuracy: 0.8286 - val_loss: 0.4669 - val_accuracy: 0.8435\n",
            "Epoch 18/500\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.5045 - accuracy: 0.8340 - val_loss: 0.5248 - val_accuracy: 0.8279\n",
            "Epoch 19/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.4931 - accuracy: 0.8368 - val_loss: 0.6093 - val_accuracy: 0.8051\n",
            "Epoch 20/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.4787 - accuracy: 0.8417 - val_loss: 0.5762 - val_accuracy: 0.8100\n",
            "Epoch 21/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.4683 - accuracy: 0.8452 - val_loss: 0.5779 - val_accuracy: 0.8123\n",
            "Epoch 22/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.4574 - accuracy: 0.8476 - val_loss: 0.5298 - val_accuracy: 0.8265\n",
            "Epoch 23/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.4544 - accuracy: 0.8477 - val_loss: 0.4784 - val_accuracy: 0.8447\n",
            "Epoch 24/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.4340 - accuracy: 0.8564 - val_loss: 0.5091 - val_accuracy: 0.8383\n",
            "Epoch 25/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.4309 - accuracy: 0.8585 - val_loss: 0.7656 - val_accuracy: 0.7663\n",
            "Epoch 26/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.4221 - accuracy: 0.8590 - val_loss: 0.4441 - val_accuracy: 0.8532\n",
            "Epoch 27/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.4162 - accuracy: 0.8617 - val_loss: 0.5082 - val_accuracy: 0.8329\n",
            "Epoch 28/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.4068 - accuracy: 0.8655 - val_loss: 0.4733 - val_accuracy: 0.8446\n",
            "Epoch 29/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3993 - accuracy: 0.8676 - val_loss: 0.4513 - val_accuracy: 0.8562\n",
            "Epoch 30/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3954 - accuracy: 0.8678 - val_loss: 0.4120 - val_accuracy: 0.8641\n",
            "Epoch 31/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3920 - accuracy: 0.8699 - val_loss: 0.5146 - val_accuracy: 0.8388\n",
            "Epoch 32/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3878 - accuracy: 0.8702 - val_loss: 0.4107 - val_accuracy: 0.8596\n",
            "Epoch 33/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3758 - accuracy: 0.8755 - val_loss: 0.7923 - val_accuracy: 0.7742\n",
            "Epoch 34/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3690 - accuracy: 0.8782 - val_loss: 0.4372 - val_accuracy: 0.8603\n",
            "Epoch 35/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3668 - accuracy: 0.8773 - val_loss: 0.4553 - val_accuracy: 0.8560\n",
            "Epoch 36/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3713 - accuracy: 0.8767 - val_loss: 0.4420 - val_accuracy: 0.8623\n",
            "Epoch 37/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3554 - accuracy: 0.8816 - val_loss: 0.4306 - val_accuracy: 0.8597\n",
            "Epoch 38/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3487 - accuracy: 0.8834 - val_loss: 0.3804 - val_accuracy: 0.8796\n",
            "Epoch 39/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3474 - accuracy: 0.8849 - val_loss: 0.4082 - val_accuracy: 0.8681\n",
            "Epoch 40/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3423 - accuracy: 0.8856 - val_loss: 0.4177 - val_accuracy: 0.8647\n",
            "Epoch 41/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3418 - accuracy: 0.8860 - val_loss: 0.3832 - val_accuracy: 0.8771\n",
            "Epoch 42/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3249 - accuracy: 0.8917 - val_loss: 0.4640 - val_accuracy: 0.8559\n",
            "Epoch 43/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3315 - accuracy: 0.8905 - val_loss: 0.4608 - val_accuracy: 0.8635\n",
            "Epoch 44/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3299 - accuracy: 0.8900 - val_loss: 0.4515 - val_accuracy: 0.8601\n",
            "Epoch 45/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3203 - accuracy: 0.8936 - val_loss: 0.5107 - val_accuracy: 0.8484\n",
            "Epoch 46/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3184 - accuracy: 0.8926 - val_loss: 0.3789 - val_accuracy: 0.8759\n",
            "Epoch 47/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3171 - accuracy: 0.8945 - val_loss: 0.3695 - val_accuracy: 0.8847\n",
            "Epoch 48/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3161 - accuracy: 0.8947 - val_loss: 0.3985 - val_accuracy: 0.8742\n",
            "Epoch 49/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3121 - accuracy: 0.8968 - val_loss: 0.3953 - val_accuracy: 0.8759\n",
            "Epoch 50/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3065 - accuracy: 0.8978 - val_loss: 0.4189 - val_accuracy: 0.8717\n",
            "Epoch 51/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.3085 - accuracy: 0.8961 - val_loss: 0.3666 - val_accuracy: 0.8841\n",
            "Epoch 52/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3005 - accuracy: 0.9007 - val_loss: 0.4159 - val_accuracy: 0.8743\n",
            "Epoch 53/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2982 - accuracy: 0.9007 - val_loss: 0.4014 - val_accuracy: 0.8748\n",
            "Epoch 54/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2912 - accuracy: 0.9032 - val_loss: 0.4699 - val_accuracy: 0.8565\n",
            "Epoch 55/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2925 - accuracy: 0.9023 - val_loss: 0.4075 - val_accuracy: 0.8781\n",
            "Epoch 56/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2902 - accuracy: 0.9029 - val_loss: 0.4430 - val_accuracy: 0.8631\n",
            "Epoch 57/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2832 - accuracy: 0.9057 - val_loss: 0.3992 - val_accuracy: 0.8765\n",
            "Epoch 58/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2859 - accuracy: 0.9046 - val_loss: 0.4232 - val_accuracy: 0.8728\n",
            "Epoch 59/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2806 - accuracy: 0.9074 - val_loss: 0.4100 - val_accuracy: 0.8780\n",
            "Epoch 60/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2783 - accuracy: 0.9062 - val_loss: 0.3618 - val_accuracy: 0.8847\n",
            "Epoch 61/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2717 - accuracy: 0.9096 - val_loss: 0.4490 - val_accuracy: 0.8739\n",
            "Epoch 62/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2730 - accuracy: 0.9088 - val_loss: 0.4037 - val_accuracy: 0.8776\n",
            "Epoch 63/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2688 - accuracy: 0.9109 - val_loss: 0.3683 - val_accuracy: 0.8809\n",
            "Epoch 64/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2690 - accuracy: 0.9106 - val_loss: 0.3798 - val_accuracy: 0.8829\n",
            "Epoch 65/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2673 - accuracy: 0.9104 - val_loss: 0.4199 - val_accuracy: 0.8709\n",
            "Epoch 66/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2652 - accuracy: 0.9094 - val_loss: 0.3564 - val_accuracy: 0.8892\n",
            "Epoch 67/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2635 - accuracy: 0.9111 - val_loss: 0.3520 - val_accuracy: 0.8951\n",
            "Epoch 68/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2551 - accuracy: 0.9143 - val_loss: 0.4064 - val_accuracy: 0.8795\n",
            "Epoch 69/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2590 - accuracy: 0.9136 - val_loss: 0.3980 - val_accuracy: 0.8845\n",
            "Epoch 70/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2587 - accuracy: 0.9130 - val_loss: 0.3362 - val_accuracy: 0.8950\n",
            "Epoch 71/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2555 - accuracy: 0.9147 - val_loss: 0.3585 - val_accuracy: 0.8887\n",
            "Epoch 72/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2511 - accuracy: 0.9172 - val_loss: 0.3857 - val_accuracy: 0.8801\n",
            "Epoch 73/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2451 - accuracy: 0.9168 - val_loss: 0.3443 - val_accuracy: 0.8949\n",
            "Epoch 74/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2448 - accuracy: 0.9179 - val_loss: 0.4209 - val_accuracy: 0.8783\n",
            "Epoch 75/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2462 - accuracy: 0.9166 - val_loss: 0.3969 - val_accuracy: 0.8846\n",
            "Epoch 76/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2471 - accuracy: 0.9169 - val_loss: 0.3765 - val_accuracy: 0.8850\n",
            "Epoch 77/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2445 - accuracy: 0.9186 - val_loss: 0.3487 - val_accuracy: 0.8931\n",
            "Epoch 78/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2458 - accuracy: 0.9183 - val_loss: 0.3694 - val_accuracy: 0.8897\n",
            "Epoch 79/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2402 - accuracy: 0.9189 - val_loss: 0.3722 - val_accuracy: 0.8882\n",
            "Epoch 80/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2355 - accuracy: 0.9209 - val_loss: 0.3674 - val_accuracy: 0.8885\n",
            "Epoch 81/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2377 - accuracy: 0.9207 - val_loss: 0.3376 - val_accuracy: 0.8972\n",
            "Epoch 82/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2348 - accuracy: 0.9221 - val_loss: 0.3719 - val_accuracy: 0.8933\n",
            "Epoch 83/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2310 - accuracy: 0.9232 - val_loss: 0.3740 - val_accuracy: 0.8862\n",
            "Epoch 84/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2323 - accuracy: 0.9232 - val_loss: 0.3636 - val_accuracy: 0.8908\n",
            "Epoch 85/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2365 - accuracy: 0.9214 - val_loss: 0.3824 - val_accuracy: 0.8901\n",
            "Epoch 86/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2250 - accuracy: 0.9250 - val_loss: 0.4341 - val_accuracy: 0.8842\n",
            "Epoch 87/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2274 - accuracy: 0.9236 - val_loss: 0.3649 - val_accuracy: 0.8944\n",
            "Epoch 88/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2280 - accuracy: 0.9226 - val_loss: 0.3498 - val_accuracy: 0.8944\n",
            "Epoch 89/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2282 - accuracy: 0.9239 - val_loss: 0.3416 - val_accuracy: 0.8990\n",
            "Epoch 90/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2241 - accuracy: 0.9268 - val_loss: 0.3734 - val_accuracy: 0.8927\n",
            "Epoch 91/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2198 - accuracy: 0.9247 - val_loss: 0.3483 - val_accuracy: 0.8970\n",
            "Epoch 92/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2185 - accuracy: 0.9273 - val_loss: 0.3581 - val_accuracy: 0.8963\n",
            "Epoch 93/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2233 - accuracy: 0.9260 - val_loss: 0.3547 - val_accuracy: 0.8985\n",
            "Epoch 94/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2205 - accuracy: 0.9265 - val_loss: 0.3737 - val_accuracy: 0.8936\n",
            "Epoch 95/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2125 - accuracy: 0.9280 - val_loss: 0.3454 - val_accuracy: 0.8969\n",
            "Epoch 96/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2121 - accuracy: 0.9275 - val_loss: 0.4511 - val_accuracy: 0.8735\n",
            "Epoch 97/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2121 - accuracy: 0.9293 - val_loss: 0.3633 - val_accuracy: 0.8942\n",
            "Epoch 98/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2194 - accuracy: 0.9273 - val_loss: 0.3772 - val_accuracy: 0.8910\n",
            "Epoch 99/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2079 - accuracy: 0.9313 - val_loss: 0.3661 - val_accuracy: 0.8930\n",
            "Epoch 100/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2106 - accuracy: 0.9298 - val_loss: 0.3783 - val_accuracy: 0.8954\n",
            "Epoch 101/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2096 - accuracy: 0.9289 - val_loss: 0.4072 - val_accuracy: 0.8903\n",
            "Epoch 102/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2104 - accuracy: 0.9307 - val_loss: 0.3814 - val_accuracy: 0.8891\n",
            "Epoch 103/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2077 - accuracy: 0.9289 - val_loss: 0.3813 - val_accuracy: 0.8924\n",
            "Epoch 104/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2059 - accuracy: 0.9311 - val_loss: 0.4364 - val_accuracy: 0.8828\n",
            "Epoch 105/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2089 - accuracy: 0.9301 - val_loss: 0.4166 - val_accuracy: 0.8822\n",
            "Epoch 106/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2050 - accuracy: 0.9326 - val_loss: 0.3694 - val_accuracy: 0.8928\n",
            "Epoch 107/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2048 - accuracy: 0.9329 - val_loss: 0.4077 - val_accuracy: 0.8830\n",
            "Epoch 108/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2011 - accuracy: 0.9344 - val_loss: 0.3809 - val_accuracy: 0.8923\n",
            "Epoch 109/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2038 - accuracy: 0.9317 - val_loss: 0.4428 - val_accuracy: 0.8755\n",
            "Epoch 110/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2016 - accuracy: 0.9332 - val_loss: 0.4099 - val_accuracy: 0.8866\n",
            "Epoch 111/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1956 - accuracy: 0.9343 - val_loss: 0.3583 - val_accuracy: 0.8951\n",
            "Epoch 112/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1955 - accuracy: 0.9344 - val_loss: 0.4267 - val_accuracy: 0.8856\n",
            "Epoch 113/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1962 - accuracy: 0.9349 - val_loss: 0.4321 - val_accuracy: 0.8831\n",
            "Epoch 114/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1967 - accuracy: 0.9355 - val_loss: 0.3962 - val_accuracy: 0.8905\n",
            "Epoch 115/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1953 - accuracy: 0.9350 - val_loss: 0.3395 - val_accuracy: 0.8971\n",
            "Epoch 116/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1944 - accuracy: 0.9356 - val_loss: 0.3554 - val_accuracy: 0.8965\n",
            "Epoch 117/500\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.1929 - accuracy: 0.9358 - val_loss: 0.3580 - val_accuracy: 0.8981\n",
            "Epoch 118/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1929 - accuracy: 0.9354 - val_loss: 0.3577 - val_accuracy: 0.8974\n",
            "Epoch 119/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1950 - accuracy: 0.9348 - val_loss: 0.3750 - val_accuracy: 0.8911\n",
            "Epoch 120/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1934 - accuracy: 0.9363 - val_loss: 0.3576 - val_accuracy: 0.9014\n",
            "Epoch 121/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1926 - accuracy: 0.9357 - val_loss: 0.3610 - val_accuracy: 0.9031\n",
            "Epoch 122/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1853 - accuracy: 0.9395 - val_loss: 0.3707 - val_accuracy: 0.8983\n",
            "Epoch 123/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1897 - accuracy: 0.9370 - val_loss: 0.3588 - val_accuracy: 0.8978\n",
            "Epoch 124/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1851 - accuracy: 0.9398 - val_loss: 0.3566 - val_accuracy: 0.8999\n",
            "Epoch 125/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1820 - accuracy: 0.9395 - val_loss: 0.3912 - val_accuracy: 0.8959\n",
            "Epoch 126/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1821 - accuracy: 0.9395 - val_loss: 0.4206 - val_accuracy: 0.8910\n",
            "Epoch 127/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1811 - accuracy: 0.9396 - val_loss: 0.3807 - val_accuracy: 0.8969\n",
            "Epoch 128/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1849 - accuracy: 0.9374 - val_loss: 0.4008 - val_accuracy: 0.8895\n",
            "Epoch 129/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1834 - accuracy: 0.9393 - val_loss: 0.3862 - val_accuracy: 0.8990\n",
            "Epoch 130/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1849 - accuracy: 0.9373 - val_loss: 0.3497 - val_accuracy: 0.9017\n",
            "Epoch 131/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1806 - accuracy: 0.9403 - val_loss: 0.4321 - val_accuracy: 0.8833\n",
            "Epoch 132/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1752 - accuracy: 0.9422 - val_loss: 0.3871 - val_accuracy: 0.8965\n",
            "Epoch 133/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1794 - accuracy: 0.9413 - val_loss: 0.3415 - val_accuracy: 0.9032\n",
            "Epoch 134/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1756 - accuracy: 0.9409 - val_loss: 0.4393 - val_accuracy: 0.8794\n",
            "Epoch 135/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1851 - accuracy: 0.9375 - val_loss: 0.4044 - val_accuracy: 0.8918\n",
            "Epoch 136/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1782 - accuracy: 0.9418 - val_loss: 0.3419 - val_accuracy: 0.9060\n",
            "Epoch 137/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1756 - accuracy: 0.9417 - val_loss: 0.4015 - val_accuracy: 0.8911\n",
            "Epoch 138/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1760 - accuracy: 0.9429 - val_loss: 0.4335 - val_accuracy: 0.8820\n",
            "Epoch 139/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1743 - accuracy: 0.9427 - val_loss: 0.3599 - val_accuracy: 0.9012\n",
            "Epoch 140/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1728 - accuracy: 0.9427 - val_loss: 0.3361 - val_accuracy: 0.9036\n",
            "Epoch 141/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1751 - accuracy: 0.9419 - val_loss: 0.3828 - val_accuracy: 0.8981\n",
            "Epoch 142/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1747 - accuracy: 0.9417 - val_loss: 0.4098 - val_accuracy: 0.8900\n",
            "Epoch 143/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1757 - accuracy: 0.9425 - val_loss: 0.3359 - val_accuracy: 0.9058\n",
            "Epoch 144/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1719 - accuracy: 0.9431 - val_loss: 0.3593 - val_accuracy: 0.9060\n",
            "Epoch 145/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1715 - accuracy: 0.9427 - val_loss: 0.3667 - val_accuracy: 0.9005\n",
            "Epoch 146/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1694 - accuracy: 0.9436 - val_loss: 0.3791 - val_accuracy: 0.9009\n",
            "Epoch 147/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1685 - accuracy: 0.9438 - val_loss: 0.3816 - val_accuracy: 0.8982\n",
            "Epoch 148/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1695 - accuracy: 0.9432 - val_loss: 0.3374 - val_accuracy: 0.9040\n",
            "Epoch 149/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1722 - accuracy: 0.9425 - val_loss: 0.4061 - val_accuracy: 0.8931\n",
            "Epoch 150/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1679 - accuracy: 0.9445 - val_loss: 0.3648 - val_accuracy: 0.9028\n",
            "Epoch 151/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1687 - accuracy: 0.9445 - val_loss: 0.3580 - val_accuracy: 0.9018\n",
            "Epoch 152/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1693 - accuracy: 0.9444 - val_loss: 0.3430 - val_accuracy: 0.9028\n",
            "Epoch 153/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1739 - accuracy: 0.9431 - val_loss: 0.3601 - val_accuracy: 0.9013\n",
            "Epoch 154/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1614 - accuracy: 0.9455 - val_loss: 0.3482 - val_accuracy: 0.9035\n",
            "Epoch 155/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1690 - accuracy: 0.9439 - val_loss: 0.3815 - val_accuracy: 0.8975\n",
            "Epoch 156/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1671 - accuracy: 0.9456 - val_loss: 0.3794 - val_accuracy: 0.8994\n",
            "Epoch 157/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1647 - accuracy: 0.9444 - val_loss: 0.3864 - val_accuracy: 0.8997\n",
            "Epoch 158/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1654 - accuracy: 0.9462 - val_loss: 0.3667 - val_accuracy: 0.9010\n",
            "Epoch 159/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1595 - accuracy: 0.9462 - val_loss: 0.3484 - val_accuracy: 0.9092\n",
            "Epoch 160/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1646 - accuracy: 0.9461 - val_loss: 0.3728 - val_accuracy: 0.9030\n",
            "Epoch 161/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1572 - accuracy: 0.9477 - val_loss: 0.3529 - val_accuracy: 0.9044\n",
            "Epoch 162/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1595 - accuracy: 0.9485 - val_loss: 0.3620 - val_accuracy: 0.9028\n",
            "Epoch 163/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1619 - accuracy: 0.9471 - val_loss: 0.3469 - val_accuracy: 0.9083\n",
            "Epoch 164/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1574 - accuracy: 0.9484 - val_loss: 0.3772 - val_accuracy: 0.9052\n",
            "Epoch 165/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1546 - accuracy: 0.9490 - val_loss: 0.3356 - val_accuracy: 0.9006\n",
            "Epoch 166/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1546 - accuracy: 0.9483 - val_loss: 0.3772 - val_accuracy: 0.9053\n",
            "Epoch 167/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1553 - accuracy: 0.9479 - val_loss: 0.3948 - val_accuracy: 0.8986\n",
            "Epoch 168/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1610 - accuracy: 0.9474 - val_loss: 0.3722 - val_accuracy: 0.9016\n",
            "Epoch 169/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1581 - accuracy: 0.9482 - val_loss: 0.3577 - val_accuracy: 0.9027\n",
            "Epoch 170/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1567 - accuracy: 0.9484 - val_loss: 0.3696 - val_accuracy: 0.9035\n",
            "Epoch 171/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1568 - accuracy: 0.9495 - val_loss: 0.3670 - val_accuracy: 0.9059\n",
            "Epoch 172/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1529 - accuracy: 0.9498 - val_loss: 0.3614 - val_accuracy: 0.9052\n",
            "Epoch 173/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1554 - accuracy: 0.9496 - val_loss: 0.3733 - val_accuracy: 0.9013\n",
            "Epoch 174/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1490 - accuracy: 0.9505 - val_loss: 0.3803 - val_accuracy: 0.8994\n",
            "Epoch 175/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1589 - accuracy: 0.9477 - val_loss: 0.3565 - val_accuracy: 0.9029\n",
            "Epoch 176/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1534 - accuracy: 0.9488 - val_loss: 0.3630 - val_accuracy: 0.9045\n",
            "Epoch 177/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1529 - accuracy: 0.9489 - val_loss: 0.3699 - val_accuracy: 0.9030\n",
            "Epoch 178/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1567 - accuracy: 0.9480 - val_loss: 0.3559 - val_accuracy: 0.9046\n",
            "Epoch 179/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1538 - accuracy: 0.9486 - val_loss: 0.3737 - val_accuracy: 0.8990\n",
            "Epoch 180/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1442 - accuracy: 0.9513 - val_loss: 0.4235 - val_accuracy: 0.8964\n",
            "Epoch 181/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1511 - accuracy: 0.9498 - val_loss: 0.3903 - val_accuracy: 0.8968\n",
            "Epoch 182/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1495 - accuracy: 0.9511 - val_loss: 0.3736 - val_accuracy: 0.9012\n",
            "Epoch 183/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1443 - accuracy: 0.9520 - val_loss: 0.3894 - val_accuracy: 0.8992\n",
            "Epoch 184/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1457 - accuracy: 0.9523 - val_loss: 0.3978 - val_accuracy: 0.8983\n",
            "Epoch 185/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1469 - accuracy: 0.9523 - val_loss: 0.4017 - val_accuracy: 0.8985\n",
            "Epoch 186/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1547 - accuracy: 0.9500 - val_loss: 0.3923 - val_accuracy: 0.8983\n",
            "Epoch 187/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1445 - accuracy: 0.9527 - val_loss: 0.3688 - val_accuracy: 0.9070\n",
            "Epoch 188/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1492 - accuracy: 0.9521 - val_loss: 0.3569 - val_accuracy: 0.9080\n",
            "Epoch 189/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1505 - accuracy: 0.9499 - val_loss: 0.3846 - val_accuracy: 0.9056\n",
            "Epoch 190/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1496 - accuracy: 0.9505 - val_loss: 0.3196 - val_accuracy: 0.9069\n",
            "Epoch 191/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1443 - accuracy: 0.9524 - val_loss: 0.4556 - val_accuracy: 0.8874\n",
            "Epoch 192/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1473 - accuracy: 0.9520 - val_loss: 0.3804 - val_accuracy: 0.8986\n",
            "Epoch 193/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1446 - accuracy: 0.9524 - val_loss: 0.3545 - val_accuracy: 0.9061\n",
            "Epoch 194/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1432 - accuracy: 0.9518 - val_loss: 0.3839 - val_accuracy: 0.9040\n",
            "Epoch 195/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1413 - accuracy: 0.9538 - val_loss: 0.3943 - val_accuracy: 0.8986\n",
            "Epoch 196/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1433 - accuracy: 0.9514 - val_loss: 0.3719 - val_accuracy: 0.9006\n",
            "Epoch 197/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1441 - accuracy: 0.9534 - val_loss: 0.3795 - val_accuracy: 0.9032\n",
            "Epoch 198/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1411 - accuracy: 0.9538 - val_loss: 0.3988 - val_accuracy: 0.9018\n",
            "Epoch 199/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1400 - accuracy: 0.9536 - val_loss: 0.3737 - val_accuracy: 0.9016\n",
            "Epoch 200/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1429 - accuracy: 0.9532 - val_loss: 0.4225 - val_accuracy: 0.8941\n",
            "Epoch 201/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1417 - accuracy: 0.9534 - val_loss: 0.3706 - val_accuracy: 0.9015\n",
            "Epoch 202/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1427 - accuracy: 0.9531 - val_loss: 0.3394 - val_accuracy: 0.9111\n",
            "Epoch 203/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1466 - accuracy: 0.9518 - val_loss: 0.3667 - val_accuracy: 0.9070\n",
            "Epoch 204/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1427 - accuracy: 0.9528 - val_loss: 0.3548 - val_accuracy: 0.9056\n",
            "Epoch 205/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1423 - accuracy: 0.9528 - val_loss: 0.4163 - val_accuracy: 0.8994\n",
            "Epoch 206/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1369 - accuracy: 0.9538 - val_loss: 0.3640 - val_accuracy: 0.9030\n",
            "Epoch 207/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1398 - accuracy: 0.9544 - val_loss: 0.3376 - val_accuracy: 0.9047\n",
            "Epoch 208/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1409 - accuracy: 0.9543 - val_loss: 0.4710 - val_accuracy: 0.8827\n",
            "Epoch 209/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1416 - accuracy: 0.9542 - val_loss: 0.4003 - val_accuracy: 0.8983\n",
            "Epoch 210/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1388 - accuracy: 0.9540 - val_loss: 0.3701 - val_accuracy: 0.9065\n",
            "Epoch 211/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1441 - accuracy: 0.9531 - val_loss: 0.3567 - val_accuracy: 0.9068\n",
            "Epoch 212/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1388 - accuracy: 0.9545 - val_loss: 0.3623 - val_accuracy: 0.9066\n",
            "Epoch 213/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1368 - accuracy: 0.9557 - val_loss: 0.3591 - val_accuracy: 0.9108\n",
            "Epoch 214/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1378 - accuracy: 0.9535 - val_loss: 0.3586 - val_accuracy: 0.9042\n",
            "Epoch 215/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1413 - accuracy: 0.9539 - val_loss: 0.3900 - val_accuracy: 0.8988\n",
            "Epoch 216/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1327 - accuracy: 0.9564 - val_loss: 0.3643 - val_accuracy: 0.9080\n",
            "Epoch 217/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1373 - accuracy: 0.9549 - val_loss: 0.3573 - val_accuracy: 0.9075\n",
            "Epoch 218/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1337 - accuracy: 0.9566 - val_loss: 0.3733 - val_accuracy: 0.9046\n",
            "Epoch 219/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1346 - accuracy: 0.9560 - val_loss: 0.3513 - val_accuracy: 0.9084\n",
            "Epoch 220/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1351 - accuracy: 0.9564 - val_loss: 0.3604 - val_accuracy: 0.9060\n",
            "Epoch 221/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1376 - accuracy: 0.9546 - val_loss: 0.3641 - val_accuracy: 0.9067\n",
            "Epoch 222/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1338 - accuracy: 0.9566 - val_loss: 0.4039 - val_accuracy: 0.8995\n",
            "Epoch 223/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1377 - accuracy: 0.9552 - val_loss: 0.3710 - val_accuracy: 0.9050\n",
            "Epoch 224/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1331 - accuracy: 0.9559 - val_loss: 0.3768 - val_accuracy: 0.9037\n",
            "Epoch 225/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1359 - accuracy: 0.9573 - val_loss: 0.3824 - val_accuracy: 0.9007\n",
            "Epoch 226/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1338 - accuracy: 0.9560 - val_loss: 0.3630 - val_accuracy: 0.9081\n",
            "Epoch 227/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1373 - accuracy: 0.9558 - val_loss: 0.3642 - val_accuracy: 0.9065\n",
            "Epoch 228/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1305 - accuracy: 0.9570 - val_loss: 0.3646 - val_accuracy: 0.9059\n",
            "Epoch 229/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1301 - accuracy: 0.9573 - val_loss: 0.3505 - val_accuracy: 0.9123\n",
            "Epoch 230/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1355 - accuracy: 0.9555 - val_loss: 0.3730 - val_accuracy: 0.9046\n",
            "Epoch 231/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1330 - accuracy: 0.9564 - val_loss: 0.4125 - val_accuracy: 0.8991\n",
            "Epoch 232/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1257 - accuracy: 0.9585 - val_loss: 0.3938 - val_accuracy: 0.9063\n",
            "Epoch 233/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1304 - accuracy: 0.9575 - val_loss: 0.4063 - val_accuracy: 0.9032\n",
            "Epoch 234/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1332 - accuracy: 0.9573 - val_loss: 0.3808 - val_accuracy: 0.9033\n",
            "Epoch 235/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1321 - accuracy: 0.9561 - val_loss: 0.4047 - val_accuracy: 0.9025\n",
            "Epoch 236/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1287 - accuracy: 0.9584 - val_loss: 0.3910 - val_accuracy: 0.9027\n",
            "Epoch 237/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1320 - accuracy: 0.9576 - val_loss: 0.4075 - val_accuracy: 0.8979\n",
            "Epoch 238/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1293 - accuracy: 0.9581 - val_loss: 0.3593 - val_accuracy: 0.9090\n",
            "Epoch 239/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1298 - accuracy: 0.9576 - val_loss: 0.3534 - val_accuracy: 0.9089\n",
            "Epoch 240/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1293 - accuracy: 0.9578 - val_loss: 0.3982 - val_accuracy: 0.8999\n",
            "Epoch 241/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1313 - accuracy: 0.9575 - val_loss: 0.4081 - val_accuracy: 0.8980\n",
            "Epoch 242/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1286 - accuracy: 0.9582 - val_loss: 0.4124 - val_accuracy: 0.9004\n",
            "Epoch 243/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1206 - accuracy: 0.9604 - val_loss: 0.3538 - val_accuracy: 0.9046\n",
            "Epoch 244/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1302 - accuracy: 0.9579 - val_loss: 0.3563 - val_accuracy: 0.9066\n",
            "Epoch 245/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1211 - accuracy: 0.9602 - val_loss: 0.3670 - val_accuracy: 0.9089\n",
            "Epoch 246/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1279 - accuracy: 0.9592 - val_loss: 0.4413 - val_accuracy: 0.8898\n",
            "Epoch 247/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1270 - accuracy: 0.9578 - val_loss: 0.3881 - val_accuracy: 0.8982\n",
            "Epoch 248/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1253 - accuracy: 0.9584 - val_loss: 0.4075 - val_accuracy: 0.9065\n",
            "Epoch 249/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1317 - accuracy: 0.9571 - val_loss: 0.3568 - val_accuracy: 0.9075\n",
            "Epoch 250/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1222 - accuracy: 0.9596 - val_loss: 0.3605 - val_accuracy: 0.9046\n",
            "Epoch 251/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1206 - accuracy: 0.9614 - val_loss: 0.3687 - val_accuracy: 0.9068\n",
            "Epoch 252/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1264 - accuracy: 0.9588 - val_loss: 0.3909 - val_accuracy: 0.9071\n",
            "Epoch 253/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1227 - accuracy: 0.9599 - val_loss: 0.3717 - val_accuracy: 0.9043\n",
            "Epoch 254/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1229 - accuracy: 0.9602 - val_loss: 0.3877 - val_accuracy: 0.9035\n",
            "Epoch 255/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1233 - accuracy: 0.9601 - val_loss: 0.3851 - val_accuracy: 0.9043\n",
            "Epoch 256/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1263 - accuracy: 0.9599 - val_loss: 0.4238 - val_accuracy: 0.8984\n",
            "Epoch 257/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1259 - accuracy: 0.9586 - val_loss: 0.3937 - val_accuracy: 0.9029\n",
            "Epoch 258/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1253 - accuracy: 0.9592 - val_loss: 0.4214 - val_accuracy: 0.8988\n",
            "Epoch 259/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1179 - accuracy: 0.9615 - val_loss: 0.3879 - val_accuracy: 0.9055\n",
            "Epoch 260/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1233 - accuracy: 0.9594 - val_loss: 0.3693 - val_accuracy: 0.9019\n",
            "Epoch 261/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1246 - accuracy: 0.9597 - val_loss: 0.3625 - val_accuracy: 0.9109\n",
            "Epoch 262/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1300 - accuracy: 0.9573 - val_loss: 0.3953 - val_accuracy: 0.8997\n",
            "Epoch 263/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1172 - accuracy: 0.9620 - val_loss: 0.3993 - val_accuracy: 0.9034\n",
            "Epoch 264/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 0.4119 - val_accuracy: 0.9030\n",
            "Epoch 265/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1227 - accuracy: 0.9603 - val_loss: 0.3863 - val_accuracy: 0.9031\n",
            "Epoch 266/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1219 - accuracy: 0.9599 - val_loss: 0.3820 - val_accuracy: 0.9035\n",
            "Epoch 267/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1159 - accuracy: 0.9632 - val_loss: 0.3783 - val_accuracy: 0.9058\n",
            "Epoch 268/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1198 - accuracy: 0.9609 - val_loss: 0.3727 - val_accuracy: 0.9100\n",
            "Epoch 269/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1201 - accuracy: 0.9615 - val_loss: 0.4129 - val_accuracy: 0.9070\n",
            "Epoch 270/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1213 - accuracy: 0.9607 - val_loss: 0.3757 - val_accuracy: 0.9084\n",
            "Epoch 271/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1240 - accuracy: 0.9587 - val_loss: 0.3824 - val_accuracy: 0.9097\n",
            "Epoch 272/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1168 - accuracy: 0.9622 - val_loss: 0.3692 - val_accuracy: 0.9111\n",
            "Epoch 273/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1228 - accuracy: 0.9596 - val_loss: 0.3745 - val_accuracy: 0.9145\n",
            "Epoch 274/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1189 - accuracy: 0.9616 - val_loss: 0.3979 - val_accuracy: 0.9071\n",
            "Epoch 275/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1210 - accuracy: 0.9614 - val_loss: 0.3756 - val_accuracy: 0.9122\n",
            "Epoch 276/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1165 - accuracy: 0.9627 - val_loss: 0.3937 - val_accuracy: 0.9074\n",
            "Epoch 277/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1224 - accuracy: 0.9594 - val_loss: 0.3676 - val_accuracy: 0.9082\n",
            "Epoch 278/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1187 - accuracy: 0.9612 - val_loss: 0.3605 - val_accuracy: 0.9076\n",
            "Epoch 279/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1163 - accuracy: 0.9615 - val_loss: 0.4004 - val_accuracy: 0.9052\n",
            "Epoch 280/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1184 - accuracy: 0.9618 - val_loss: 0.3818 - val_accuracy: 0.9102\n",
            "Epoch 281/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1167 - accuracy: 0.9618 - val_loss: 0.3885 - val_accuracy: 0.9075\n",
            "Epoch 282/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1152 - accuracy: 0.9631 - val_loss: 0.3562 - val_accuracy: 0.9125\n",
            "Epoch 283/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1200 - accuracy: 0.9623 - val_loss: 0.3847 - val_accuracy: 0.9043\n",
            "Epoch 284/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1174 - accuracy: 0.9622 - val_loss: 0.3935 - val_accuracy: 0.9045\n",
            "Epoch 285/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1177 - accuracy: 0.9627 - val_loss: 0.4073 - val_accuracy: 0.9029\n",
            "Epoch 286/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1201 - accuracy: 0.9619 - val_loss: 0.3761 - val_accuracy: 0.9102\n",
            "Epoch 287/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1142 - accuracy: 0.9636 - val_loss: 0.3753 - val_accuracy: 0.9108\n",
            "Epoch 288/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1191 - accuracy: 0.9607 - val_loss: 0.3860 - val_accuracy: 0.9125\n",
            "Epoch 289/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1207 - accuracy: 0.9610 - val_loss: 0.4792 - val_accuracy: 0.8938\n",
            "Epoch 290/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1237 - accuracy: 0.9603 - val_loss: 0.3867 - val_accuracy: 0.9104\n",
            "Epoch 291/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1156 - accuracy: 0.9623 - val_loss: 0.3609 - val_accuracy: 0.9110\n",
            "Epoch 292/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1120 - accuracy: 0.9633 - val_loss: 0.3526 - val_accuracy: 0.9146\n",
            "Epoch 293/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1120 - accuracy: 0.9636 - val_loss: 0.4387 - val_accuracy: 0.9038\n",
            "Epoch 294/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1171 - accuracy: 0.9625 - val_loss: 0.3765 - val_accuracy: 0.9108\n",
            "Epoch 295/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1194 - accuracy: 0.9613 - val_loss: 0.3685 - val_accuracy: 0.9097\n",
            "Epoch 296/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1124 - accuracy: 0.9628 - val_loss: 0.4485 - val_accuracy: 0.9012\n",
            "Epoch 297/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.1223 - accuracy: 0.9601 - val_loss: 0.3506 - val_accuracy: 0.9117\n",
            "Epoch 298/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1156 - accuracy: 0.9625 - val_loss: 0.3833 - val_accuracy: 0.9069\n",
            "Epoch 299/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1128 - accuracy: 0.9635 - val_loss: 0.3993 - val_accuracy: 0.9048\n",
            "Epoch 300/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1128 - accuracy: 0.9629 - val_loss: 0.3709 - val_accuracy: 0.9098\n",
            "Epoch 301/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1113 - accuracy: 0.9634 - val_loss: 0.3950 - val_accuracy: 0.9036\n",
            "Epoch 302/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1135 - accuracy: 0.9636 - val_loss: 0.4233 - val_accuracy: 0.9072\n",
            "Epoch 303/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1130 - accuracy: 0.9640 - val_loss: 0.4121 - val_accuracy: 0.9057\n",
            "Epoch 304/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1133 - accuracy: 0.9633 - val_loss: 0.3789 - val_accuracy: 0.9098\n",
            "Epoch 305/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1154 - accuracy: 0.9627 - val_loss: 0.3825 - val_accuracy: 0.9068\n",
            "Epoch 306/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1109 - accuracy: 0.9649 - val_loss: 0.4104 - val_accuracy: 0.9062\n",
            "Epoch 307/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.1125 - accuracy: 0.9623 - val_loss: 0.4072 - val_accuracy: 0.9048\n",
            "Epoch 308/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.1139 - accuracy: 0.9634 - val_loss: 0.3612 - val_accuracy: 0.9072\n",
            "Epoch 309/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1115 - accuracy: 0.9632 - val_loss: 0.4280 - val_accuracy: 0.9010\n",
            "Epoch 310/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1145 - accuracy: 0.9628 - val_loss: 0.3733 - val_accuracy: 0.9109\n",
            "Epoch 311/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1093 - accuracy: 0.9647 - val_loss: 0.3916 - val_accuracy: 0.9077\n",
            "Epoch 312/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1096 - accuracy: 0.9639 - val_loss: 0.4205 - val_accuracy: 0.9049\n",
            "Epoch 313/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1100 - accuracy: 0.9648 - val_loss: 0.3646 - val_accuracy: 0.9117\n",
            "Epoch 314/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.1105 - accuracy: 0.9641 - val_loss: 0.3568 - val_accuracy: 0.9180\n",
            "Epoch 315/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1092 - accuracy: 0.9645 - val_loss: 0.3689 - val_accuracy: 0.9101\n",
            "Epoch 316/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1140 - accuracy: 0.9633 - val_loss: 0.3506 - val_accuracy: 0.9128\n",
            "Epoch 317/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1078 - accuracy: 0.9649 - val_loss: 0.3532 - val_accuracy: 0.9146\n",
            "Epoch 318/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1071 - accuracy: 0.9650 - val_loss: 0.3509 - val_accuracy: 0.9103\n",
            "Epoch 319/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1166 - accuracy: 0.9626 - val_loss: 0.3674 - val_accuracy: 0.9109\n",
            "Epoch 320/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1114 - accuracy: 0.9639 - val_loss: 0.3811 - val_accuracy: 0.9119\n",
            "Epoch 321/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1106 - accuracy: 0.9643 - val_loss: 0.3813 - val_accuracy: 0.9097\n",
            "Epoch 322/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1126 - accuracy: 0.9634 - val_loss: 0.4002 - val_accuracy: 0.9037\n",
            "Epoch 323/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1077 - accuracy: 0.9657 - val_loss: 0.3748 - val_accuracy: 0.9092\n",
            "Epoch 324/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1107 - accuracy: 0.9642 - val_loss: 0.3842 - val_accuracy: 0.9074\n",
            "Epoch 325/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1075 - accuracy: 0.9642 - val_loss: 0.3573 - val_accuracy: 0.9141\n",
            "Epoch 326/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1077 - accuracy: 0.9646 - val_loss: 0.3800 - val_accuracy: 0.9128\n",
            "Epoch 327/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1114 - accuracy: 0.9643 - val_loss: 0.3793 - val_accuracy: 0.9130\n",
            "Epoch 328/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1120 - accuracy: 0.9639 - val_loss: 0.3708 - val_accuracy: 0.9105\n",
            "Epoch 329/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1088 - accuracy: 0.9646 - val_loss: 0.4116 - val_accuracy: 0.9045\n",
            "Epoch 330/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1098 - accuracy: 0.9639 - val_loss: 0.4025 - val_accuracy: 0.9093\n",
            "Epoch 331/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1102 - accuracy: 0.9645 - val_loss: 0.4215 - val_accuracy: 0.9084\n",
            "Epoch 332/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1105 - accuracy: 0.9644 - val_loss: 0.3983 - val_accuracy: 0.9085\n",
            "Epoch 333/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1080 - accuracy: 0.9650 - val_loss: 0.3891 - val_accuracy: 0.9057\n",
            "Epoch 334/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1076 - accuracy: 0.9646 - val_loss: 0.3983 - val_accuracy: 0.9109\n",
            "Epoch 335/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.1078 - accuracy: 0.9654 - val_loss: 0.4151 - val_accuracy: 0.9026\n",
            "Epoch 336/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1034 - accuracy: 0.9672 - val_loss: 0.3811 - val_accuracy: 0.9128\n",
            "Epoch 337/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1069 - accuracy: 0.9660 - val_loss: 0.3870 - val_accuracy: 0.9051\n",
            "Epoch 338/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1115 - accuracy: 0.9641 - val_loss: 0.3975 - val_accuracy: 0.9058\n",
            "Epoch 339/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1064 - accuracy: 0.9656 - val_loss: 0.3671 - val_accuracy: 0.9129\n",
            "Epoch 340/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1129 - accuracy: 0.9632 - val_loss: 0.3504 - val_accuracy: 0.9151\n",
            "Epoch 341/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1042 - accuracy: 0.9657 - val_loss: 0.4041 - val_accuracy: 0.9119\n",
            "Epoch 342/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1056 - accuracy: 0.9661 - val_loss: 0.3869 - val_accuracy: 0.9112\n",
            "Epoch 343/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1050 - accuracy: 0.9667 - val_loss: 0.3886 - val_accuracy: 0.9124\n",
            "Epoch 344/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1039 - accuracy: 0.9675 - val_loss: 0.3704 - val_accuracy: 0.9158\n",
            "Epoch 345/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1039 - accuracy: 0.9668 - val_loss: 0.3817 - val_accuracy: 0.9105\n",
            "Epoch 346/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1049 - accuracy: 0.9665 - val_loss: 0.3735 - val_accuracy: 0.9115\n",
            "Epoch 347/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1030 - accuracy: 0.9675 - val_loss: 0.3914 - val_accuracy: 0.9128\n",
            "Epoch 348/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1044 - accuracy: 0.9668 - val_loss: 0.4073 - val_accuracy: 0.9069\n",
            "Epoch 349/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1084 - accuracy: 0.9654 - val_loss: 0.3787 - val_accuracy: 0.9100\n",
            "Epoch 350/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1026 - accuracy: 0.9663 - val_loss: 0.3789 - val_accuracy: 0.9090\n",
            "Epoch 351/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1029 - accuracy: 0.9664 - val_loss: 0.4009 - val_accuracy: 0.9102\n",
            "Epoch 352/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1042 - accuracy: 0.9667 - val_loss: 0.4226 - val_accuracy: 0.9067\n",
            "Epoch 353/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1084 - accuracy: 0.9654 - val_loss: 0.3843 - val_accuracy: 0.9103\n",
            "Epoch 354/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1061 - accuracy: 0.9650 - val_loss: 0.3768 - val_accuracy: 0.9104\n",
            "Epoch 355/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1031 - accuracy: 0.9676 - val_loss: 0.4163 - val_accuracy: 0.9051\n",
            "Epoch 356/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1031 - accuracy: 0.9670 - val_loss: 0.3830 - val_accuracy: 0.9103\n",
            "Epoch 357/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1038 - accuracy: 0.9673 - val_loss: 0.4104 - val_accuracy: 0.9116\n",
            "Epoch 358/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1058 - accuracy: 0.9660 - val_loss: 0.3839 - val_accuracy: 0.9093\n",
            "Epoch 359/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1041 - accuracy: 0.9667 - val_loss: 0.3832 - val_accuracy: 0.9116\n",
            "Epoch 360/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1038 - accuracy: 0.9668 - val_loss: 0.3914 - val_accuracy: 0.9107\n",
            "Epoch 361/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1036 - accuracy: 0.9668 - val_loss: 0.3633 - val_accuracy: 0.9142\n",
            "Epoch 362/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1021 - accuracy: 0.9668 - val_loss: 0.4047 - val_accuracy: 0.9107\n",
            "Epoch 363/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1004 - accuracy: 0.9670 - val_loss: 0.4080 - val_accuracy: 0.9077\n",
            "Epoch 364/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1033 - accuracy: 0.9667 - val_loss: 0.3995 - val_accuracy: 0.9079\n",
            "Epoch 365/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1008 - accuracy: 0.9683 - val_loss: 0.3865 - val_accuracy: 0.9090\n",
            "Epoch 366/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1005 - accuracy: 0.9684 - val_loss: 0.4472 - val_accuracy: 0.9032\n",
            "Epoch 367/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1017 - accuracy: 0.9679 - val_loss: 0.3750 - val_accuracy: 0.9087\n",
            "Epoch 368/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0983 - accuracy: 0.9682 - val_loss: 0.4455 - val_accuracy: 0.8973\n",
            "Epoch 369/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0963 - accuracy: 0.9688 - val_loss: 0.4113 - val_accuracy: 0.9061\n",
            "Epoch 370/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1016 - accuracy: 0.9674 - val_loss: 0.4142 - val_accuracy: 0.9074\n",
            "Epoch 371/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0976 - accuracy: 0.9687 - val_loss: 0.3837 - val_accuracy: 0.9118\n",
            "Epoch 372/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1015 - accuracy: 0.9677 - val_loss: 0.3957 - val_accuracy: 0.9078\n",
            "Epoch 373/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0977 - accuracy: 0.9678 - val_loss: 0.3836 - val_accuracy: 0.9132\n",
            "Epoch 374/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1044 - accuracy: 0.9658 - val_loss: 0.3574 - val_accuracy: 0.9123\n",
            "Epoch 375/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1028 - accuracy: 0.9671 - val_loss: 0.4084 - val_accuracy: 0.9057\n",
            "Epoch 376/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0974 - accuracy: 0.9688 - val_loss: 0.3829 - val_accuracy: 0.9105\n",
            "Epoch 377/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0975 - accuracy: 0.9687 - val_loss: 0.3637 - val_accuracy: 0.9152\n",
            "Epoch 378/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1027 - accuracy: 0.9668 - val_loss: 0.3616 - val_accuracy: 0.9146\n",
            "Epoch 379/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1015 - accuracy: 0.9677 - val_loss: 0.4295 - val_accuracy: 0.9063\n",
            "Epoch 380/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0992 - accuracy: 0.9685 - val_loss: 0.3931 - val_accuracy: 0.9118\n",
            "Epoch 381/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1015 - accuracy: 0.9672 - val_loss: 0.3907 - val_accuracy: 0.9087\n",
            "Epoch 382/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1027 - accuracy: 0.9675 - val_loss: 0.4003 - val_accuracy: 0.9120\n",
            "Epoch 383/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0952 - accuracy: 0.9695 - val_loss: 0.3707 - val_accuracy: 0.9151\n",
            "Epoch 384/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0993 - accuracy: 0.9685 - val_loss: 0.3616 - val_accuracy: 0.9122\n",
            "Epoch 385/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1003 - accuracy: 0.9671 - val_loss: 0.3872 - val_accuracy: 0.9113\n",
            "Epoch 386/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0969 - accuracy: 0.9689 - val_loss: 0.3990 - val_accuracy: 0.9114\n",
            "Epoch 387/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0980 - accuracy: 0.9683 - val_loss: 0.3839 - val_accuracy: 0.9112\n",
            "Epoch 388/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0998 - accuracy: 0.9671 - val_loss: 0.4717 - val_accuracy: 0.8971\n",
            "Epoch 389/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0988 - accuracy: 0.9669 - val_loss: 0.4134 - val_accuracy: 0.9082\n",
            "Epoch 390/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0965 - accuracy: 0.9685 - val_loss: 0.3991 - val_accuracy: 0.9132\n",
            "Epoch 391/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0985 - accuracy: 0.9682 - val_loss: 0.3836 - val_accuracy: 0.9095\n",
            "Epoch 392/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0977 - accuracy: 0.9686 - val_loss: 0.3638 - val_accuracy: 0.9130\n",
            "Epoch 393/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0979 - accuracy: 0.9693 - val_loss: 0.3792 - val_accuracy: 0.9065\n",
            "Epoch 394/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0992 - accuracy: 0.9680 - val_loss: 0.3926 - val_accuracy: 0.9128\n",
            "Epoch 395/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0943 - accuracy: 0.9698 - val_loss: 0.4198 - val_accuracy: 0.9092\n",
            "Epoch 396/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0939 - accuracy: 0.9700 - val_loss: 0.3699 - val_accuracy: 0.9169\n",
            "Epoch 397/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0953 - accuracy: 0.9681 - val_loss: 0.4027 - val_accuracy: 0.9033\n",
            "Epoch 398/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1000 - accuracy: 0.9677 - val_loss: 0.4002 - val_accuracy: 0.9090\n",
            "Epoch 399/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0988 - accuracy: 0.9680 - val_loss: 0.3813 - val_accuracy: 0.9101\n",
            "Epoch 400/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0938 - accuracy: 0.9696 - val_loss: 0.3923 - val_accuracy: 0.9139\n",
            "Epoch 401/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0981 - accuracy: 0.9683 - val_loss: 0.3627 - val_accuracy: 0.9128\n",
            "Epoch 402/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0988 - accuracy: 0.9686 - val_loss: 0.4272 - val_accuracy: 0.9025\n",
            "Epoch 403/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0974 - accuracy: 0.9689 - val_loss: 0.3660 - val_accuracy: 0.9133\n",
            "Epoch 404/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0939 - accuracy: 0.9708 - val_loss: 0.3628 - val_accuracy: 0.9153\n",
            "Epoch 405/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.1006 - accuracy: 0.9682 - val_loss: 0.3677 - val_accuracy: 0.9109\n",
            "Epoch 406/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0937 - accuracy: 0.9704 - val_loss: 0.3603 - val_accuracy: 0.9152\n",
            "Epoch 407/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0941 - accuracy: 0.9704 - val_loss: 0.3894 - val_accuracy: 0.9070\n",
            "Epoch 408/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.1015 - accuracy: 0.9671 - val_loss: 0.3536 - val_accuracy: 0.9152\n",
            "Epoch 409/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0959 - accuracy: 0.9699 - val_loss: 0.4174 - val_accuracy: 0.9034\n",
            "Epoch 410/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0921 - accuracy: 0.9706 - val_loss: 0.3913 - val_accuracy: 0.9087\n",
            "Epoch 411/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0938 - accuracy: 0.9692 - val_loss: 0.4045 - val_accuracy: 0.9109\n",
            "Epoch 412/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0964 - accuracy: 0.9697 - val_loss: 0.3644 - val_accuracy: 0.9140\n",
            "Epoch 413/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0956 - accuracy: 0.9701 - val_loss: 0.3856 - val_accuracy: 0.9101\n",
            "Epoch 414/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 0.4172 - val_accuracy: 0.9074\n",
            "Epoch 415/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0931 - accuracy: 0.9707 - val_loss: 0.4098 - val_accuracy: 0.9066\n",
            "Epoch 416/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0945 - accuracy: 0.9700 - val_loss: 0.4202 - val_accuracy: 0.9074\n",
            "Epoch 417/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0931 - accuracy: 0.9704 - val_loss: 0.3684 - val_accuracy: 0.9099\n",
            "Epoch 418/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0940 - accuracy: 0.9693 - val_loss: 0.4029 - val_accuracy: 0.9097\n",
            "Epoch 419/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0907 - accuracy: 0.9693 - val_loss: 0.4314 - val_accuracy: 0.9086\n",
            "Epoch 420/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0908 - accuracy: 0.9707 - val_loss: 0.3788 - val_accuracy: 0.9125\n",
            "Epoch 421/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0936 - accuracy: 0.9704 - val_loss: 0.4029 - val_accuracy: 0.9064\n",
            "Epoch 422/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0945 - accuracy: 0.9703 - val_loss: 0.4099 - val_accuracy: 0.9121\n",
            "Epoch 423/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0963 - accuracy: 0.9683 - val_loss: 0.3959 - val_accuracy: 0.9107\n",
            "Epoch 424/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0963 - accuracy: 0.9693 - val_loss: 0.3627 - val_accuracy: 0.9140\n",
            "Epoch 425/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0923 - accuracy: 0.9701 - val_loss: 0.3927 - val_accuracy: 0.9121\n",
            "Epoch 426/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0920 - accuracy: 0.9707 - val_loss: 0.4454 - val_accuracy: 0.9037\n",
            "Epoch 427/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0934 - accuracy: 0.9701 - val_loss: 0.4172 - val_accuracy: 0.9082\n",
            "Epoch 428/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0945 - accuracy: 0.9700 - val_loss: 0.3707 - val_accuracy: 0.9115\n",
            "Epoch 429/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0962 - accuracy: 0.9692 - val_loss: 0.4005 - val_accuracy: 0.9093\n",
            "Epoch 430/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0877 - accuracy: 0.9720 - val_loss: 0.4065 - val_accuracy: 0.9121\n",
            "Epoch 431/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0956 - accuracy: 0.9692 - val_loss: 0.3701 - val_accuracy: 0.9139\n",
            "Epoch 432/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0907 - accuracy: 0.9704 - val_loss: 0.4079 - val_accuracy: 0.9104\n",
            "Epoch 433/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0908 - accuracy: 0.9702 - val_loss: 0.4011 - val_accuracy: 0.9129\n",
            "Epoch 434/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0944 - accuracy: 0.9696 - val_loss: 0.3741 - val_accuracy: 0.9144\n",
            "Epoch 435/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0928 - accuracy: 0.9704 - val_loss: 0.3590 - val_accuracy: 0.9141\n",
            "Epoch 436/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0896 - accuracy: 0.9706 - val_loss: 0.3811 - val_accuracy: 0.9103\n",
            "Epoch 437/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0953 - accuracy: 0.9686 - val_loss: 0.3907 - val_accuracy: 0.9119\n",
            "Epoch 438/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0899 - accuracy: 0.9704 - val_loss: 0.3840 - val_accuracy: 0.9125\n",
            "Epoch 439/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0868 - accuracy: 0.9722 - val_loss: 0.4060 - val_accuracy: 0.9121\n",
            "Epoch 440/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0933 - accuracy: 0.9700 - val_loss: 0.3791 - val_accuracy: 0.9124\n",
            "Epoch 441/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0918 - accuracy: 0.9707 - val_loss: 0.4284 - val_accuracy: 0.9082\n",
            "Epoch 442/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0960 - accuracy: 0.9693 - val_loss: 0.4153 - val_accuracy: 0.9058\n",
            "Epoch 443/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0936 - accuracy: 0.9702 - val_loss: 0.3894 - val_accuracy: 0.9171\n",
            "Epoch 444/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0885 - accuracy: 0.9714 - val_loss: 0.4145 - val_accuracy: 0.9116\n",
            "Epoch 445/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0936 - accuracy: 0.9701 - val_loss: 0.3863 - val_accuracy: 0.9125\n",
            "Epoch 446/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0859 - accuracy: 0.9723 - val_loss: 0.4272 - val_accuracy: 0.9146\n",
            "Epoch 447/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0958 - accuracy: 0.9708 - val_loss: 0.4193 - val_accuracy: 0.9104\n",
            "Epoch 448/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0965 - accuracy: 0.9694 - val_loss: 0.3841 - val_accuracy: 0.9098\n",
            "Epoch 449/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0914 - accuracy: 0.9700 - val_loss: 0.4599 - val_accuracy: 0.9046\n",
            "Epoch 450/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0904 - accuracy: 0.9714 - val_loss: 0.3882 - val_accuracy: 0.9165\n",
            "Epoch 451/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0921 - accuracy: 0.9706 - val_loss: 0.3779 - val_accuracy: 0.9155\n",
            "Epoch 452/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0898 - accuracy: 0.9709 - val_loss: 0.4353 - val_accuracy: 0.9103\n",
            "Epoch 453/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0896 - accuracy: 0.9714 - val_loss: 0.3673 - val_accuracy: 0.9134\n",
            "Epoch 454/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0949 - accuracy: 0.9694 - val_loss: 0.3581 - val_accuracy: 0.9138\n",
            "Epoch 455/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0858 - accuracy: 0.9719 - val_loss: 0.4122 - val_accuracy: 0.9072\n",
            "Epoch 456/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0886 - accuracy: 0.9722 - val_loss: 0.4161 - val_accuracy: 0.9090\n",
            "Epoch 457/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0874 - accuracy: 0.9717 - val_loss: 0.4673 - val_accuracy: 0.9006\n",
            "Epoch 458/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0899 - accuracy: 0.9706 - val_loss: 0.4196 - val_accuracy: 0.9058\n",
            "Epoch 459/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0891 - accuracy: 0.9720 - val_loss: 0.3907 - val_accuracy: 0.9099\n",
            "Epoch 460/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0899 - accuracy: 0.9718 - val_loss: 0.4150 - val_accuracy: 0.9070\n",
            "Epoch 461/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0937 - accuracy: 0.9702 - val_loss: 0.3822 - val_accuracy: 0.9115\n",
            "Epoch 462/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0868 - accuracy: 0.9719 - val_loss: 0.4141 - val_accuracy: 0.9097\n",
            "Epoch 463/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0907 - accuracy: 0.9701 - val_loss: 0.4357 - val_accuracy: 0.9107\n",
            "Epoch 464/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0974 - accuracy: 0.9699 - val_loss: 0.4041 - val_accuracy: 0.9054\n",
            "Epoch 465/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0889 - accuracy: 0.9711 - val_loss: 0.4121 - val_accuracy: 0.9106\n",
            "Epoch 466/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0895 - accuracy: 0.9708 - val_loss: 0.4120 - val_accuracy: 0.9117\n",
            "Epoch 467/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0903 - accuracy: 0.9726 - val_loss: 0.3920 - val_accuracy: 0.9132\n",
            "Epoch 468/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0861 - accuracy: 0.9727 - val_loss: 0.3808 - val_accuracy: 0.9134\n",
            "Epoch 469/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0913 - accuracy: 0.9704 - val_loss: 0.3921 - val_accuracy: 0.9137\n",
            "Epoch 470/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0860 - accuracy: 0.9725 - val_loss: 0.3947 - val_accuracy: 0.9147\n",
            "Epoch 471/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0941 - accuracy: 0.9698 - val_loss: 0.3959 - val_accuracy: 0.9134\n",
            "Epoch 472/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0908 - accuracy: 0.9718 - val_loss: 0.4015 - val_accuracy: 0.9129\n",
            "Epoch 473/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0899 - accuracy: 0.9717 - val_loss: 0.4247 - val_accuracy: 0.9118\n",
            "Epoch 474/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0889 - accuracy: 0.9718 - val_loss: 0.3814 - val_accuracy: 0.9143\n",
            "Epoch 475/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0883 - accuracy: 0.9721 - val_loss: 0.4309 - val_accuracy: 0.9082\n",
            "Epoch 476/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0884 - accuracy: 0.9715 - val_loss: 0.4193 - val_accuracy: 0.9094\n",
            "Epoch 477/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0905 - accuracy: 0.9717 - val_loss: 0.4289 - val_accuracy: 0.9047\n",
            "Epoch 478/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0858 - accuracy: 0.9729 - val_loss: 0.4506 - val_accuracy: 0.9083\n",
            "Epoch 479/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0869 - accuracy: 0.9725 - val_loss: 0.3902 - val_accuracy: 0.9097\n",
            "Epoch 480/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0864 - accuracy: 0.9713 - val_loss: 0.3995 - val_accuracy: 0.9146\n",
            "Epoch 481/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0848 - accuracy: 0.9730 - val_loss: 0.3895 - val_accuracy: 0.9121\n",
            "Epoch 482/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0849 - accuracy: 0.9739 - val_loss: 0.4491 - val_accuracy: 0.9100\n",
            "Epoch 483/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0928 - accuracy: 0.9708 - val_loss: 0.3838 - val_accuracy: 0.9137\n",
            "Epoch 484/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0891 - accuracy: 0.9713 - val_loss: 0.4060 - val_accuracy: 0.9095\n",
            "Epoch 485/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0868 - accuracy: 0.9720 - val_loss: 0.3839 - val_accuracy: 0.9114\n",
            "Epoch 486/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0875 - accuracy: 0.9718 - val_loss: 0.4102 - val_accuracy: 0.9121\n",
            "Epoch 487/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0872 - accuracy: 0.9730 - val_loss: 0.4124 - val_accuracy: 0.9118\n",
            "Epoch 488/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0878 - accuracy: 0.9723 - val_loss: 0.4215 - val_accuracy: 0.9125\n",
            "Epoch 489/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0859 - accuracy: 0.9721 - val_loss: 0.3920 - val_accuracy: 0.9128\n",
            "Epoch 490/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0863 - accuracy: 0.9716 - val_loss: 0.4099 - val_accuracy: 0.9101\n",
            "Epoch 491/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0832 - accuracy: 0.9730 - val_loss: 0.4187 - val_accuracy: 0.9122\n",
            "Epoch 492/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0860 - accuracy: 0.9721 - val_loss: 0.4169 - val_accuracy: 0.9135\n",
            "Epoch 493/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0892 - accuracy: 0.9720 - val_loss: 0.3920 - val_accuracy: 0.9149\n",
            "Epoch 494/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0900 - accuracy: 0.9706 - val_loss: 0.4014 - val_accuracy: 0.9077\n",
            "Epoch 495/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0849 - accuracy: 0.9733 - val_loss: 0.3872 - val_accuracy: 0.9132\n",
            "Epoch 496/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0858 - accuracy: 0.9727 - val_loss: 0.4164 - val_accuracy: 0.9129\n",
            "Epoch 497/500\n",
            "625/625 [==============================] - 18s 30ms/step - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.3809 - val_accuracy: 0.9138\n",
            "Epoch 498/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0835 - accuracy: 0.9734 - val_loss: 0.4274 - val_accuracy: 0.9098\n",
            "Epoch 499/500\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.0860 - accuracy: 0.9726 - val_loss: 0.3921 - val_accuracy: 0.9140\n",
            "Epoch 500/500\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.3890 - val_accuracy: 0.9152\n",
            "313/313 - 1s - loss: 0.4192 - accuracy: 0.9042\n",
            "> 90.420\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Pm_vi_V3py"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAsiNMFfV5m8"
      },
      "source": [
        "# Classify Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "bKpMYDUeOMVx",
        "outputId": "afcd8b5d-0395-4351-992e-a9d63fcdd7c4"
      },
      "source": [
        "model_to_classify = load_model('final_model.h5')\n",
        "classify_image(model_to_classify)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff3f31de-6d46-4a68-a23d-ab92dbed6d34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff3f31de-6d46-4a68-a23d-ab92dbed6d34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving bird1.png to bird1.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We think this is of class :  bird\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hkTzIbxgagP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or_1E25YFrdA"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "## Note 1: Keras Function/Sequential API\n",
        "\n",
        "Within Keras there are two API's for coding network models: Sequential and functional. You may have already used the [seqential model](https://https://www.tensorflow.org/guide/keras/sequential_model) to code basic artifical neural networks. The [functional model](https://www.tensorflow.org/guide/keras/functional) is more general and enables us to include feedforward paths which are required to code more advanced networks such as ResNet or DenseNet. The functional API has been used to code the helper functions repeating unit/top level skeleton detailed below.\n",
        "\n",
        "## Note 2: Repeating Unit\n",
        "\n",
        "In many Deep architectures there will often be a repeating unit(s) that is composed of a sequence of layers, for example Conv,ReLU & Batch Normalisation. Specific layers can be turned on or off as required. This also allows experimentation of the entire architecture with/without specific layers (e.g. Batch Normalisation)  You may wish to create other combinations of layers or additional repeating units depending on your architecture.\n",
        "\n",
        "In the code cell below we can see an example of a `repeat_unit` function where the number of kernel filters and kernel size is specified. Batch Normalisation and activation layers can be turned on/off as required.\n",
        "\n",
        " * Convolutional Layers are described [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) \n",
        " * Batch Normalization layers are described [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) \n",
        " * Activation layers are described [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation) \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwDIngPfrww"
      },
      "source": [
        "# Example of a Basic Repeating unit\n",
        "\n",
        "def repeat_unit(inputs, num_channels=16, kernel_size=3,\n",
        "                 strides=1, activation='relu', batch_normalization=False):\n",
        "    \"\"\"\n",
        "    inputs: Input to repeat unit.\n",
        "    num_channels: number of channels in convolution layer of repeat unit, nominally = 16\n",
        "    kernel_size: number of kernels to use in convolution, nominally = 3\n",
        "    strides: number of strides for kernel, nominally = 1\n",
        "    activation: type of activation applied to output of convolution, nominally ReLU. \n",
        "    x = output of repeat_unit\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_channels, kernel_size=kernel_size, strides=strides, padding='same',\n",
        "                  kernel_initializer='he_normal')\n",
        "    \n",
        "    x = conv(inputs)\n",
        "    \n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "\n",
        "    if batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFvVHWvWagSN"
      },
      "source": [
        "## Note 3: Top Level\n",
        "\n",
        "In the code cell below, an incomplete skeleton function is provided for your top level.  \n",
        "* This uses the Keras functional API\n",
        "* A single repeating unit which does not reduce the spatial dimensions of the input is placed at the top of the stack. You can change the number of channels/kernel size to suit your own architecture.\n",
        "* You can add your own layers/repeating units etc to build your own architecture with the function.\n",
        "* At the end of the stack we add a [global average pooling layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) and fully connected [(Dense)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer with softmax activation. (Alternatively to the Global Average pooling Dense layers can be used, this approach uses more parameter (memory) and is commonly seen in earlier Deeep Learning architectures such as AlexNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwPACVhkf0uF"
      },
      "source": [
        "\n",
        "def network_top(input_shape,num_classes=10):\n",
        "    \"\"\"\n",
        "    input Shape: (Define H,W, no. channels) of network input\n",
        "                 used in Input definition below\n",
        "    num_classes: Default = 10, sets output classes of network,\n",
        "                 set to same no. of classes as CIFAR 10 Dataset \n",
        "    model:       keras Model returned by this function.                          \n",
        "    \"\"\"\n",
        "       \n",
        "    # Define input of model\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Add First repeating Unit\n",
        "    x = repeat_unit(inputs=inputs, num_channels=12)\n",
        "  \n",
        "   ######## Add more layers/repeating units to build your Architecture here ##################  \n",
        "\n",
        "\n",
        "    # Add final stages. (Ensure that you call the output of the very last layer outputs)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(x)\n",
        "\n",
        "    # Model definition\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}